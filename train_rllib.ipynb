{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "from src.envs.reward_func import equity_log_return_reward\n",
    "from src.utils.data_loader import DataLoader\n",
    "from src.utils.preprocessor import Preprocessor\n",
    "from src.utils.trainer import Trainer\n",
    "from src.utils.backtest import backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"ES=F\", \"^GSPC\", \"^N225\"]\n",
    "data = DataLoader.fetch_data(\"^N225\", interval=\"1d\", start=\"2009-01-01\", end=\"2021-09-30\")\n",
    "# data = DataLoader.load_data(\"./data/LTCUSD.csv\")\n",
    "data_len = len(data)\n",
    "# _data_train = data.iloc[: int(data_len * 0.8), :]\n",
    "_data_train = data.loc[:\"2019-12-31\", :]\n",
    "# _data_eval = data.iloc[int(data_len * 0.8) :, :]\n",
    "_data_eval = data.loc[\"2020-01-01\":, :]\n",
    "print(f\"Training Sapn: {_data_train.index[0]} to {_data_train.index[-1]}, Length: {len(_data_train)}\")\n",
    "print(f\"Evaluating Span: {_data_eval.index[0]} to {_data_eval.index[-1]}, Length: {len(_data_eval)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, features_train = Preprocessor.preprocessing(_data_train)\n",
    "data_eval, features_eval = Preprocessor.preprocessing(_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train, features_train, data_eval, features_eval = Preprocessor.preprocessing(_data_train, _data_eval, use_tech_indicators=False)\n",
    "# features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "fee = 0.001\n",
    "reward_func = equity_log_return_reward\n",
    "user_config = {\n",
    "    \"env\": \"DescTradingEnv\",\n",
    "    \"env_config\": {\n",
    "        \"df\": data_train,\n",
    "        \"features\": features_train,\n",
    "        \"reward_func\": reward_func,\n",
    "        \"window_size\": window_size,\n",
    "    },\n",
    "\n",
    "    \"evaluation_num_workers\": 4,\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_num_episodes\": 10,\n",
    "    \"evaluation_config\": {\n",
    "        \"env_config\": {\n",
    "            \"df\": data_eval,\n",
    "            \"features\": features_eval,\n",
    "            \"reward_func\": reward_func,\n",
    "            \"window_size\": window_size,\n",
    "        },\n",
    "        # \"explore\": False,\n",
    "        \"explore\": True,\n",
    "    },\n",
    "\n",
    "    # \"model\": {\n",
    "    #     # By default, the MODEL_DEFAULTS dict above will be used.\n",
    "\n",
    "    #     # Change individual keys in that dict by overriding them, e.g.\n",
    "    #     \"fcnet_hiddens\": [64, 16],\n",
    "    #     \"custom_model\": \"bn_model\",\n",
    "    #     # \"vf_share_layers\": True,\n",
    "    #     # \"use_attention\": True\n",
    "    # },\n",
    "\n",
    "    \"num_workers\": 4,  # parallelism\n",
    "    \"framework\": \"torch\",\n",
    "    \"log_level\": \"WARN\",  # \"WARN\", \"DEBUG\"\n",
    "    \"seed\": 3407,\n",
    "    \"batch_mode\": \"complete_episodes\",\n",
    "    # \"observation_filter\": \"MeanStdFilter\",\n",
    "\n",
    "    # \"n_step\": 5,\n",
    "    # \"noisy\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_class, config = Trainer.get_agent_class(\"PPO\")\n",
    "config.update(user_config)\n",
    "agent = agent_class(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, last_checkpoint = Trainer.learn(agent, timesteps_total=5e5, checkpoint_freq=10)\n",
    "# agent.restore(\"./ray_results/PPO\\checkpoint_000071\\checkpoint-71\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = Trainer.get_env(config[\"env\"], config[\"env_config\"])\n",
    "env_eval = Trainer.get_env(config[\"env\"], config[\"evaluation_config\"][\"env_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = backtest(_data_eval, env_eval, agent, plot=True, plot_filename=\"PPO_backtest\")\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = pd.DataFrame()\n",
    "# stats[\"train\"] = backtest(_data_train, env_train, agent, plot=True, plot_filename=\"PPO_train_backtest\")\n",
    "# stats[\"eval\"] = backtest(_data_eval, env_eval, agent, plot=True, plot_filename=\"PPO_eval_backtest\")\n",
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# observation = env_train.reset()\n",
    "# policy_id = \"default_policy\"\n",
    "# policy = agent.get_policy(policy_id)\n",
    "# local_worker = agent.workers.local_worker()\n",
    "# pp = local_worker.preprocessors[policy_id]\n",
    "# done = False\n",
    "# while not done:\n",
    "#     obs, reward, done, _ = env_train.step(0)\n",
    "#     filtered_observation = local_worker.filters[policy_id](observation)\n",
    "#     print(local_worker.get_filters())\n",
    "# pprint(observation)\n",
    "# pprint(filtered_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import send_line_notification\n",
    "send_line_notification('Lab | Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_policy().model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92494b380cdd7d1d1ced08fe7bc7df853d898bcea1d66a2a33078fd6ace70ca7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('drl-trading': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
