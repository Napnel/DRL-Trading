{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "# sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "# sns.set_context(\"paper\", 1.5, {\"lines.linewidth\": 4})\n",
    "# sns.set_palette(\"winter_r\", 8, 1)\n",
    "# sns.set_context(\"paper\", 1.5)\n",
    "sns.set('paper','whitegrid', font_scale=3, rc={\"lines.linewidth\": 3, 'grid.linestyle': '--'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>agent_timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>...</th>\n",
       "      <th>evaluation/sampler_perf/mean_env_render_ms</th>\n",
       "      <th>info/learner/default_policy/learner_stats/allreduce_latency</th>\n",
       "      <th>info/learner/default_policy/learner_stats/cur_kl_coeff</th>\n",
       "      <th>info/learner/default_policy/learner_stats/cur_lr</th>\n",
       "      <th>info/learner/default_policy/learner_stats/total_loss</th>\n",
       "      <th>info/learner/default_policy/learner_stats/policy_loss</th>\n",
       "      <th>info/learner/default_policy/learner_stats/vf_loss</th>\n",
       "      <th>info/learner/default_policy/learner_stats/kl</th>\n",
       "      <th>info/learner/default_policy/learner_stats/entropy</th>\n",
       "      <th>info/learner/default_policy/learner_stats/entropy_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.396728</td>\n",
       "      <td>-0.814902</td>\n",
       "      <td>-0.165912</td>\n",
       "      <td>1218.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>-0.006508</td>\n",
       "      <td>0.020190</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.690883</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.485353</td>\n",
       "      <td>-1.496657</td>\n",
       "      <td>-0.559203</td>\n",
       "      <td>1218.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>-0.001541</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717079</td>\n",
       "      <td>-1.744192</td>\n",
       "      <td>-0.496537</td>\n",
       "      <td>1218.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.035579</td>\n",
       "      <td>-0.002956</td>\n",
       "      <td>0.038122</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.682406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897416</td>\n",
       "      <td>-2.117442</td>\n",
       "      <td>-0.536381</td>\n",
       "      <td>1218.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>True</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>-0.006740</td>\n",
       "      <td>0.028977</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.674871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0            0.396728           -0.814902            -0.165912   \n",
       "1            0.485353           -1.496657            -0.559203   \n",
       "2            0.717079           -1.744192            -0.496537   \n",
       "3            0.897416           -2.117442            -0.536381   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0       1218.666667                 3.0                  1.0           4000.0   \n",
       "1       1218.666667                 3.0                  1.0           8000.0   \n",
       "2       1218.666667                 3.0                  1.0          12000.0   \n",
       "3       1218.666667                 4.0                  1.0          16000.0   \n",
       "\n",
       "   agent_timesteps_total   done  episodes_total  ...  \\\n",
       "0                 4000.0  False             3.0  ...   \n",
       "1                 8000.0  False             6.0  ...   \n",
       "2                12000.0  False             9.0  ...   \n",
       "3                16000.0   True            13.0  ...   \n",
       "\n",
       "   evaluation/sampler_perf/mean_env_render_ms  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/allreduce_latency  \\\n",
       "0                                                0.0             \n",
       "1                                                0.0             \n",
       "2                                                0.0             \n",
       "3                                                0.0             \n",
       "\n",
       "   info/learner/default_policy/learner_stats/cur_kl_coeff  \\\n",
       "0                                           0.200000        \n",
       "1                                           0.100000        \n",
       "2                                           0.066667        \n",
       "3                                           0.058333        \n",
       "\n",
       "   info/learner/default_policy/learner_stats/cur_lr  \\\n",
       "0                                           0.00005   \n",
       "1                                           0.00005   \n",
       "2                                           0.00005   \n",
       "3                                           0.00005   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/total_loss  \\\n",
       "0                                           0.014132      \n",
       "1                                           0.025766      \n",
       "2                                           0.035579      \n",
       "3                                           0.022798      \n",
       "\n",
       "   info/learner/default_policy/learner_stats/policy_loss  \\\n",
       "0                                          -0.006508       \n",
       "1                                          -0.001541       \n",
       "2                                          -0.002956       \n",
       "3                                          -0.006740       \n",
       "\n",
       "   info/learner/default_policy/learner_stats/vf_loss  \\\n",
       "0                                           0.020190   \n",
       "1                                           0.026522   \n",
       "2                                           0.038122   \n",
       "3                                           0.028977   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/kl  \\\n",
       "0                                      0.002253   \n",
       "1                                      0.007863   \n",
       "2                                      0.005860   \n",
       "3                                      0.013490   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/entropy  \\\n",
       "0                                          0.690883   \n",
       "1                                          0.676396   \n",
       "2                                          0.682406   \n",
       "3                                          0.674871   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/entropy_coeff  \n",
       "0                                                0.0       \n",
       "1                                                0.0       \n",
       "2                                                0.0       \n",
       "3                                                0.0       \n",
       "\n",
       "[4 rows x 62 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_progress = {}\n",
    "\n",
    "path = os.path.join(\"./experiments/2021-11-09_14-37_DescTradingEnv/Trainer_DescTradingEnv_125c5_00000_0_2021-11-09_14-37-05\", \"progress.csv\")\n",
    "progress = pd.read_csv(path)\n",
    "# summary_progress[algo] = progress[[\"episode_reward_mean\", \"evaluation/episode_reward_mean\", \"timesteps_total\"]]\n",
    "progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['episode_reward_max', 'episode_reward_min', 'episode_reward_mean',\n",
       "       'episode_len_mean', 'episodes_this_iter', 'num_healthy_workers',\n",
       "       'timesteps_total', 'agent_timesteps_total', 'done', 'episodes_total',\n",
       "       'training_iteration', 'timestamp', 'time_this_iter_s', 'time_total_s',\n",
       "       'pid', 'time_since_restore', 'timesteps_since_restore',\n",
       "       'iterations_since_restore', 'experiment_id', 'date', 'hostname',\n",
       "       'node_ip', 'trial_id', 'sampler_perf/mean_raw_obs_processing_ms',\n",
       "       'sampler_perf/mean_inference_ms',\n",
       "       'sampler_perf/mean_action_processing_ms',\n",
       "       'sampler_perf/mean_env_wait_ms', 'sampler_perf/mean_env_render_ms',\n",
       "       'timers/sample_time_ms', 'timers/sample_throughput',\n",
       "       'timers/load_time_ms', 'timers/load_throughput', 'timers/learn_time_ms',\n",
       "       'timers/learn_throughput', 'timers/update_time_ms',\n",
       "       'info/num_steps_sampled', 'info/num_agent_steps_sampled',\n",
       "       'info/num_steps_trained', 'info/num_agent_steps_trained',\n",
       "       'evaluation/episode_reward_max', 'evaluation/episode_reward_min',\n",
       "       'evaluation/episode_reward_mean', 'evaluation/episode_len_mean',\n",
       "       'evaluation/episodes_this_iter', 'perf/cpu_util_percent',\n",
       "       'perf/ram_util_percent', 'perf/gpu_util_percent0',\n",
       "       'perf/vram_util_percent0',\n",
       "       'evaluation/sampler_perf/mean_raw_obs_processing_ms',\n",
       "       'evaluation/sampler_perf/mean_inference_ms',\n",
       "       'evaluation/sampler_perf/mean_action_processing_ms',\n",
       "       'evaluation/sampler_perf/mean_env_wait_ms',\n",
       "       'evaluation/sampler_perf/mean_env_render_ms',\n",
       "       'info/learner/default_policy/learner_stats/allreduce_latency',\n",
       "       'info/learner/default_policy/learner_stats/cur_kl_coeff',\n",
       "       'info/learner/default_policy/learner_stats/cur_lr',\n",
       "       'info/learner/default_policy/learner_stats/total_loss',\n",
       "       'info/learner/default_policy/learner_stats/policy_loss',\n",
       "       'info/learner/default_policy/learner_stats/vf_loss',\n",
       "       'info/learner/default_policy/learner_stats/kl',\n",
       "       'info/learner/default_policy/learner_stats/entropy',\n",
       "       'info/learner/default_policy/learner_stats/entropy_coeff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    192.168.1.2\n",
       "1    192.168.1.2\n",
       "2    192.168.1.2\n",
       "3    192.168.1.2\n",
       "Name: node_ip, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress[\"node_ip\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e45917c04d38222114aa5092d372c38c1fe51cd799a955b9958f10f4348c0bb7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('drl-trading': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
